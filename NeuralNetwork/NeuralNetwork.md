# ニューラルネットワーク (Neural Network)

## 基本構造
- 入力層 (Input Layer)  
    特徴量を受け取る層
- 隠れ層 (Hidden Layer)  
    データを変換し, 特徴を抽出する層
- 出力層 (Output Layer)  
    モデルの最終的な予測値を出力する層
- ユニット (unit)
    各ノードのこと, 入力を重み付き和で計算し, 活性化関数を適用する

## ユニットが1つの場合
1つのユニットにおける出力は以下のように計算される：

$$
u = \sum_{i=1}^n w_i x_i + b
$$

$$
z = f(u)
$$

- $w_i$: 各入力に対する重み  
- $x_i$: 各入力データ  
- $b$: バイアス  
- $u$: ユニット入力
- $z$: ユニット出力
- $f(u)$: 活性化関数

## ユニットが複数の場合
多層ニューラルネットワークについては以下のように計算される: 

$$
\mathbf{u}^{(\ell)} = \mathbf{W}^{(\ell-1)} \mathbf{z}^{(\ell-1)} + \mathbf{b}^{(\ell-1)}
$$

$$
\mathbf{z}^{(\ell)} = f_{\ell}(\mathbf{u}^{(\ell)})
$$

- $\mathbf{W}^{(\ell)}$: $\ell$層のパラメータ行列
- $\mathbf{b}^{(\ell)}$: $\ell$層のバイアス 
- $\mathbf{u}^{(\ell)}$: $\ell$層へのユニット入力  
- $\mathbf{z}^{(\ell)}$: $\ell$層からのユニット出力  
- $f_{\ell}$: $\ell$層における活性化関数 

## ニューラルネットワークのイメージ 
[^1]
<img width="50%" src="./NN_example.png" alt="ニューラルネットワークのイメージ">

## 順伝播
1. 入力データが層を通過し, 重み(Weights)とバイアス(Bias)を使用して計算する. 
2. 各ノードで、活性化関数(Activation Function)を適用して非線形な結果を生成する. 
3. 最後に出力層で結果を得る. 

## 活性化関数
### 恒等写像
回帰の出力層に用いる. 

$$
u = \sum_{i=1}^n w_i x_i + b
$$

$$
f(u) = u
$$

- $w_i$: 各入力に対する重み  
- $x_i$: 各入力データ  
- $b$: バイアス  
- $u$: ユニット入力
- $f(u)$: 活性化関数

### ロジスティック(シグモイド)関数
2値分類の出力層に用いる. 

$$
u = \sum_{i=1}^n w_i x_i + b
$$

$$
f(u) = \frac{1}{1 + exp(-u)}
$$

- $w_i$: 各入力に対する重み  
- $x_i$: 各入力データ  
- $b$: バイアス  
- $u$: ユニット入力
- $f(u)$: 活性化関数

以下の性質を持つ. 
- $\forall u \in \mathbb{R}, 0 < f(u) < 1$
- $\lim_{u\to \infty} f(u) = 1$
- $\lim_{u\to -\infty} f(u) = 0$
- $f(0) = 0.5$

### softmax関数
多クラス分類の出力層に用いる. 

$$
\mathbf{u} = \mathbf{W} \mathbf{x} + \mathbf{b}
$$

$$
f_k(\mathbf{u}) = \frac{exp(u_k)}{\sum^{K}_{\ell=1} exp(u_{\ell})}
$$

- $\mathbf{x}$: 入力
- $\mathbf{W}$: パラメータ行列
- $\mathbf{b}$: バイアス
- $\mathbf{u}$: ユニット入力  
- $f_k$: ベクトルのk番目に対応する活性化関数

以下の性質を持つ. 
- $\forall u \in \mathbb{R}, 0 < f_k(\mathbf{u}) < 1$
- $\sum^{K}_{k=1} f_k(\mathbf{u}) = 1$
- $K=2$のとき, シグモイド関数と一致する

$K = 2$の場合, 2つのクラスに対応する出力$u_1, u_2$に対して：

$$
f_1 = \frac{\exp(u_1)}{\exp(u_1) + \exp(u_2)} = \frac{1}{1 + \exp(-(u_1 - u_2))}
$$

これはシグモイド関数の形と一致する. 

### ReLU

## 多層NNにおける学習
- 既知事例 $\{(x_{(1)}, y_{(1)}), (x_{(2)}, y_{(2)}), \ldots, (x_{(N)}, y_{(N)})\}$
- 既知事例のラベル $y_n$ とモデル出力 $z^{(L)}$ に対して, 
  損失関数 $L(W)$ を最小化するように各層のパラメータ $W^{(\ell)}$ を学習

## 損失関数
### 回帰
二乗誤差を最小にするので
$$
L(W) = \frac{1}{N} \sum_{n=1}^N \| y_{(n)} - z^{(L)}_{(n)} \|_2^2
$$

### 二値分類
$y=1$となる最大尤度を求めるので
$$
L(W) = \sum_{n=1}^N \log(1 + \exp(-y_{(n)} z^{(L)}_{(n)}))
$$

### 多クラス分類
各ラベルが1の時の最大尤度を求めるので
$$
L(W) = -\sum_{n=1}^N \sum_{k=1}^K y_{(n)k} \log(z^{(L)}_{(n)k})
$$

## ニューラルネットワークの利点と課題
### 利点
- 大量のデータを処理できる
- 訓練データから複雑な特徴量を抽出できる

### 課題
- 訓練に多くのデータとマシンパワーが必要
- 解釈性が低い
- 過学習のリスクがある

## 過学習と対策
### 過学習(Overfitting)
訓練データに対して過度に適応し, 汎化性能が低下する現象. 

### 正則化(Regularization)
モデルの複雑さを抑える手法, 損失関数にパラメータの上昇を抑えるようなペナルティ項を追加する.  (例: L1正則化、L2正則化)

### ドロップアウト(Dropout)
学習中にランダムにユニットを非活性化して過学習を防ぐ手法. 

### 交差検証(Crossvalidation)
訓練データをk個の部分集合に分割する. k-1個を訓練データ, 残った1個のデータをテストデータとして, 学習とテストを繰り返す. 
各検証の予測誤差の平均を交差検証の予測誤差とする. 

### バッチ正則化
#### ミニバッチ学習
パラメータの更新をサンプル1つ単位で行うのではなく, 少数のサンプルをまとめその単位でパラメーターを更新する. 
- エポック: 1つの訓練データを何回繰り返して学習させるか
- バッチ回数: ミニバッチ学習を何セット行うか
- バッチサイズ: 1回の学習で抽出する学習データの量

#### バッチ正則化
ミニバッチを平均が0, 標準偏差が1となるように正則化行うことで学習を効率的にする手法. 
バッチ正則化二より, ミニバッチがランダム抽出されて毎回特徴量のスケールが異なる問題を解消する. 

## 参考文献

[^1]: [AWS ニューラルネットワークとは](https://aws.amazon.com/jp/what-is/neural-network/)
